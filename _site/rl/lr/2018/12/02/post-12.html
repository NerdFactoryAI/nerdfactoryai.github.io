<!DOCTYPE html>
<html lang="kr"><head>
 <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.5.0 -->
<title>EPISODIC CURIOSITY THROUGH REACHABILITY | NerdFactory AI Tech Blog</title>
<meta name="generator" content="Jekyll v3.7.4" />
<meta property="og:title" content="EPISODIC CURIOSITY THROUGH REACHABILITY" />
<meta name="author" content="차 금강" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="본 논문은 DeepMinds와 Google Brain이 함께 호기심에 대해서 연구한 자료입니다. 1. Introduction 많은 Task들은 Sparse한 Reward의 환경을 가지고 있다. 이 문제를 풀기 위해서 한 가지 방법은 새로운 state를 발견하는 것에 대한 보너스 점수를 주는 것이다.(Curiosity-driven Exploration by Self-supervised Prediction를 일컫는 것 같네요)" />
<meta property="og:description" content="본 논문은 DeepMinds와 Google Brain이 함께 호기심에 대해서 연구한 자료입니다. 1. Introduction 많은 Task들은 Sparse한 Reward의 환경을 가지고 있다. 이 문제를 풀기 위해서 한 가지 방법은 새로운 state를 발견하는 것에 대한 보너스 점수를 주는 것이다.(Curiosity-driven Exploration by Self-supervised Prediction를 일컫는 것 같네요)" />
<link rel="canonical" href="http://localhost:4000/rl/lr/2018/12/02/post-12.html" />
<meta property="og:url" content="http://localhost:4000/rl/lr/2018/12/02/post-12.html" />
<meta property="og:site_name" content="NerdFactory AI Tech Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-12-02T03:16:01+09:00" />
<script type="application/ld+json">
{"description":"본 논문은 DeepMinds와 Google Brain이 함께 호기심에 대해서 연구한 자료입니다. 1. Introduction 많은 Task들은 Sparse한 Reward의 환경을 가지고 있다. 이 문제를 풀기 위해서 한 가지 방법은 새로운 state를 발견하는 것에 대한 보너스 점수를 주는 것이다.(Curiosity-driven Exploration by Self-supervised Prediction를 일컫는 것 같네요)","author":{"@type":"Person","name":"차 금강"},"@type":"BlogPosting","url":"http://localhost:4000/rl/lr/2018/12/02/post-12.html","headline":"EPISODIC CURIOSITY THROUGH REACHABILITY","dateModified":"2018-12-02T03:16:01+09:00","datePublished":"2018-12-02T03:16:01+09:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/rl/lr/2018/12/02/post-12.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="NerdFactory AI Tech Blog" /><script src="/assets/javascript/bootstrap/jquery.min.js"></script>
  <script src="/assets/javascript/bootstrap/bootstrap.bundle.min.js"></script>
</head>
<body><nav class="navbar navbar-expand-lg navbar-light bg-light">
  <div class="container">
    <a class="navbar-brand" rel="author" href="/">기술블로그</a>
    <!-- <a class="navbar-brand" rel="author" href="/">NerdFactory AI Tech Blog</a> --><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent"
      aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto"><li class="nav-item">
          <a class="nav-link" href="#">홈</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="#">너드팩토리</a>
        </li>
      </ul>
    </div>
  </div>
</nav>
<div class="py-5">
      <div class="container page-content">
          <article>

  <header>
    <h1>EPISODIC CURIOSITY THROUGH REACHABILITY</h1>
    <p class="text-muted">
      <time datetime="2018-12-02T03:16:01+09:00">Dec 2, 2018
      </time>• 차 금강</p>
  </header>

  <section>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async=""></script>

<p>본 <a href="https://arxiv.org/abs/1810.02274">논문</a>은 DeepMinds와 Google Brain이 함께 호기심에 대해서 연구한 자료입니다.</p>

<h1 id="1-introduction">1. Introduction</h1>

<ul>
  <li>
    <p>많은 Task들은 Sparse한 Reward의 환경을 가지고 있다.</p>
  </li>
  <li>
    <p>이 문제를 풀기 위해서 한 가지 방법은 새로운 state를 발견하는 것에 대한 보너스 점수를 주는 것이다.(Curiosity-driven Exploration by Self-supervised Prediction를 일컫는 것 같네요)</p>
  </li>
</ul>

<!--more-->

<ul>
  <li>
    <p>많은 현대적 방법들은 surprise를 공식화 하는 것에 집중을 하고 있다. 이것이 미래를 예측하는 것은 아니다. 그래서 이것은 충분히 이해할만한 방법론이지만 완벽하진 않다.</p>
  </li>
  <li>
    <p>Curiosity에 대한 새로운 정의와 수식을 제안한다.</p>
  </li>
  <li>
    <p>새로운 Curiosity에 대한 예) state의 변화에 대해 다음 state를 예측할 수 있을 때 짧은 스텝만에 새로운 state로 도달할 수 있다면 이것은 진정한 의미의 Curiosity가 아니다. TV를 예로 든다면 버튼을 누르는 action을 했을 때 한 스텝만에 TV의 화면이 바뀌기 때문에 이것은 Curiosity로 지정하지 않는다.</p>
  </li>
  <li>
    <p>위의 예로 부터 새로운 환경에 도달하는데 까지 걸리는 step의 수를 effort(노력)이라고 하며 오직 이 척도로부터만 Curiosity의 정도가 측정된다.</p>
  </li>
  <li>
    <p>이것을 측정하기 위해 neural network를 사용한다: 2개의 observations가 들어가고 두 개의 observations가 몇 스텝의 간격을 가지고 있는지(하나의 observation에서 다른 observation으로 도달하는데 까지 몇 개의 스텝을 소요해야하는지)를 측정.</p>
  </li>
  <li>
    <p>이를 novelty via reachability라고 일컫는다. 이것에 대한 도식은 아래의 그림.</p>
  </li>
</ul>

<p align="center">
  <img src="/images/EPISODIC CURIOSITY THROUGH REACHABILITY/image1.png" />
</p>

<ul>
  <li>Practically implemenetation하기 위해서 이미 탐색한 state들을 메모리에 보유하고 있어야 한다. 다음 현재의 observation이 novel하다면(neural network가 출력하는 값, state간의 스텝이 클 경우) 에이전트는 보너스 reward를 받으며 memory에 추가합니다. 이 프로세스가 에피소드가 끝날 때까지 진행합니다.</li>
</ul>

<h1 id="2-episodic-curiosity">2. Episodic Curiosity</h1>

<p>기본적인 환경과 상호작용하는 RL 에이전트를 생각해본다. 이 환경은 sparse하게 reward를 제공하기에 대부분의 time step t에서 reward가 0을 가진다. PPO와 같은 현재 널리 쓰이고 있는 에이전트들은 잘 작동하지 않는다. 우리는 Episodic Curiosity(EC라고 줄임)라는 것을 적용하여 기존의 RL 알고리즘들의 성능을 증폭시킬 수 있는 방법을 제시한다. 우리는 bonus점수 $b_t$를 생성하는 것을 목표로 한다. 그래서 에이전트가 받는 $\hat{r}_t$는 $r_t+b_t$로 한다.</p>

<h1 id="21-episodic-curiosity-module">2.1 Episodic Curiosity Module</h1>

<p>EC 모듈은 현재 observation $o_t$를 입력으로 하고 reward bonus $b_t$를 출력으로 한다. 이 모듈은 parametric한 부분과 non-parametric한 부분 모두를 포함한다.</p>

<p>2개의 parametric 부분을 가지고 있다. 1) embedding network $E : O \rightarrow R^n$ 2) comparator network $C : R^n \times R^n \rightarrow [0, 1]$</p>

<p>2개의 parametric 부분은 reachability network의 한 부분으로써 추론하기 위해 함께 학습한다. 아래의 그림에서 볼 수 있듯이.</p>

<p align="center">
  <img src="/images/EPISODIC CURIOSITY THROUGH REACHABILITY/image2.png" />
</p>

<p>2개의 non-parameteric 부분은 다음과 같다. 1) episodic memory buffur $M$ 2) reward bonus estination function $B$</p>

<p>EC 모듈의 Overview는 아래의 그림과 같다.</p>

<p align="center">
  <img src="/images/EPISODIC CURIOSITY THROUGH REACHABILITY/image3.png" />
</p>

<ul>
  <li>
    <p><strong>Embedding and comparator networks</strong> : 두 개의 네트워크는 observation $o_i$로 부터 $o_j$에 도달하는데까지 걸리는 k-step-reachability를 측정하는데 사용되며 이를 수식적으로 표현하면 $R(o_i,o_j)=C(E(o_i), E(o_j))$가 된다. Figure 2에서 볼 수 있듯이 R-network는 logistic regression loss로 학습된 classifier이다. 특정 k 단계안에 $o_i$로부터 $o_j$로 도달할 수 있으면 0을 출력하고 그렇지 않으면 1을 출력한다. 이 네트워크는 R-network로 칭한다.</p>
  </li>
  <li>
    <p><strong>Episodic memory</strong> : 현재의 에피소드에서 과거 observation들의 embedding정보들을 모으는 buffer M(embedding network E로 연산됩니다)가 있다. Memory buffer M은 메모리와 퍼포먼스 이슈를 해결하기 위해 K 사이즈로 제한된다. 매 스텝에서, 현재 obervation의 embedding 정보가 메모리에 저장된다. 만약 K 사이즈가 넘어간다면 메모리에서 무작위로 하나의 embedding 정보를 제거하고 현재 observation의 embedding 정보로 대체한다. 이 방법으로 오래 전 스텝의 정보가 메모리에서 사라지고 새로운 정보로 대체될 것이지만 무작위로 샘플링을 하기에 오래 전의 스텝 정보가 무조건적으로 사라지진 않는다.</p>
  </li>
  <li>
    <p><strong>Reward bonus estimation module</strong> : 이 모듈의 목적은 현재 observation에 대해서 reachability를 측정하고 현재 메모리 안에 그 정보가 있지 않다면 큰 bonus reward를 할당한다. reachability는 comparator network를 이용하여 현재 메모리 내에 있는 embedding 정보와 비교하는 것으로 측정된다.</p>
  </li>
</ul>

<h1 id="22-bonus-computation-algorithm">2.2 Bonus Computation Algorithm</h1>

<ul>
  <li>
    <p>매 스텝에서, embedding network를 이용하여 현재 observation $o$에 대한 embedding vector($e=E(o)$)를 가져온다.
그 embedding vector($e$)는 메모리 버퍼 $M=&lt;e_1,e_2,…,e_{|M|}&gt;$에 있는 embedding 정보들과 comparator network C를 이용하여 비교한다.</p>
  </li>
  <li>
    <p>이 comparator network는 reachability buffer를 $c_i=C(e,e_i)$의 값으로 채운다.
그리고 reachability buffer안에 있는 모든 $c_i$들을 이용하여 novelty를 측정한다.</p>
  </li>
</ul>

<script type="math/tex; mode=display">C(C,e) = F(c_1, ..., c_{|M|})</script>

<ul>
  <li>여기서 F는 어떠한 함수이다. 이론적으로 F=max과 같은 함수를 사용하는 것도 좋은 방법이다. 하지만 이것을 이용하면 parametric embedding과 comparator network에서 outliers가 발생한다. 경험적으로 그 값의 90%를 사용하면 잘 작동한다. Curiosity bonus로 $b = B(M,e) = \alpha(\beta - C(M, e))$ 이며 $\alpha$와 $\beta$는 하이퍼파라미터이다. 본 논문에서는 $\beta=0.5$를 사용했고 그러므로써 $b$는 $[-\dfrac{\alpha}{2}, \dfrac{\alpha}{2}]$의 범위를 가지게 된다. 그리고 $\alpha$는 task의 성격에 따라서 달라진다. bonus reward를 계산하고 난 후 observation embedding이 만약 특정한 값($b_{novelty}$)보다 높다면 memory에 넣는다(메모리, 데이터 중복 등등에 의해서).</li>
</ul>

<h1 id="23-reachability-network-training">2.3 Reachability Network Training</h1>

<ul>
  <li>
    <p>만약 Figure 1과 같은 아키텍쳐가 가능하다면, reachability network가 필요하지 않을 것이고 해석적으로 shortest-path algorithm으로 novelty가 계산이 될 수 있다. 하지만 일반적으로 우리는 하나의 경로에 대한 sequence를 얻을 수 있다. 다행히도, 간단한 sequence에 대해서도 training이 가능하다.</p>
  </li>
  <li>
    <p>이 방법은 Figure 2에 도식화 되어 있다. 이 방법은 $o_1, …, o_N$ 의 일련의 observations들을 입력으로 가지며 그것들을 쌍으로 가진다.</p>
  </li>
  <li>
    <p>쌍 $(o_i, o_j)$ 는 $\rvert i-j \rvert$ &lt; $k$ 이면 reachable하다고 판단하고 positive한 값을 가진다.</p>
  </li>
  <li>
    <p>$\rvert i-j \rvert$ &gt; $\gamma k$ 에는 negative한 값을 가진다. 하이퍼 파라미터 $\gamma$ 는 positive와 negative한 값 사이의 gap을 만들어내는데 필요하다.</p>
  </li>
  <li>
    <p>결국 네트워크는 reachability를 계산하기 위해 logistic regression loss를 학습한다.</p>
  </li>
</ul>

  </section>

</article>

      </div>
    </div>
<div class="py-5 border-top">
  <div class="container">

    <p class="h5 mb-3">NerdFactory AI Tech Blog</p>

    <div class="row">

      <div class="col-sm">
        <ul class="list-unstyled">
          <li>NerdFactory AI Tech Blog</li><li>
              <a href="mailto:hwang.jongtaek@gmail.com">
                hwang.jongtaek@gmail.com
              </a>
            </li></ul>
      </div>


      <div class="col-sm text-right">
        <p>Tech Blog Page for Nerd Factory AI</p>

        <p>
          <a href="https://nicolas-van.github.io/bootstrap-4-github-pages/">Powered by Bootstrap 4 Github Pages</a>
        </p>
      </div>

    </div>

  </div>
</div>
</body>

</html>
