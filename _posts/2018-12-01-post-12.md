---
layout: post
title:  "EPISODIC CURIOSITY THROUGH REACHABILITY"
author: "차 금강"
date:   2018-12-01 12:16:01 -0600
categories: ["RL", "LR"]
tags: ["tag 1"]
excerpt_separator: <!--more-->
---

<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>

본 [논문](https://arxiv.org/abs/1810.02274)은 DeepMinds와 Google Brain이 함께 호기심에 대해서 연구한 자료입니다. 

# 1. Introduction

* 많은 Task들은 Sparse한 Reward의 환경을 가지고 있다.

* 이 문제를 풀기 위해서 한 가지 방법은 새로운 state를 발견하는 것에 대한 보너스 점수를 주는 것이다.(Curiosity-driven Exploration by Self-supervised Prediction를 일컫는 것 같네요)

<!--more-->

* 많은 현대적 방법들은 surprise를 공식화 하는 것에 집중을 하고 있다. 이것이 미래를 예측하는 것은 아니다. 그래서 이것은 충분히 이해할만한 방법론이지만 완벽하진 않다.

* Curiosity에 대한 새로운 정의와 수식을 제안한다.

* 새로운 Curiosity에 대한 예) state의 변화에 대해 다음 state를 예측할 수 있을 때 짧은 스텝만에 새로운 state로 도달할 수 있다면 이것은 진정한 의미의 Curiosity가 아니다. TV를 예로 든다면 버튼을 누르는 action을 했을 때 한 스텝만에 TV의 화면이 바뀌기 때문에 이것은 Curiosity로 지정하지 않는다.

* 위의 예로 부터 새로운 환경에 도달하는데 까지 걸리는 step의 수를 effort(노력)이라고 하며 오직 이 척도로부터만 Curiosity의 정도가 측정된다.

* 이것을 측정하기 위해 neural network를 사용한다: 2개의 observations가 들어가고 두 개의 observations가 몇 스텝의 간격을 가지고 있는지(하나의 observation에서 다른 observation으로 도달하는데 까지 몇 개의 스텝을 소요해야하는지)를 측정.

* 이를 novelty via reachability라고 일컫는다. 이것에 대한 도식은 아래의 그림.

<p align="center">
  <img src="/images/EPISODIC CURIOSITY THROUGH REACHABILITY/image1.png">
</p>

* Practically implemenetation하기 위해서 이미 탐색한 state들을 메모리에 보유하고 있어야 한다. 다음 현재의 observation이 novel하다면(neural network가 출력하는 값, state간의 스텝이 클 경우) 에이전트는 보너스 reward를 받으며 memory에 추가합니다. 이 프로세스가 에피소드가 끝날 때까지 진행합니다.

# 2. Episodic Curiosity

기본적인 환경과 상호작용하는 RL 에이전트를 생각해본다. 이 환경은 sparse하게 reward를 제공하기에 대부분의 time step t에서 reward가 0을 가진다. PPO와 같은 현재 널리 쓰이고 있는 에이전트들은 잘 작동하지 않는다. 우리는 Episodic Curiosity(EC라고 줄임)라는 것을 적용하여 기존의 RL 알고리즘들의 성능을 증폭시킬 수 있는 방법을 제시한다. 우리는 bonus점수 $b_t$를 생성하는 것을 목표로 한다. 그래서 에이전트가 받는 $\hat{r}_t$는 $r_t+b_t$로 한다.

# 2.1 Episodic Curiosity Module

EC 모듈은 현재 observation $o_t$를 입력으로 하고 reward bonus $b_t$를 출력으로 한다. 이 모듈은 parametric한 부분과 non-parametric한 부분 모두를 포함한다. 

2개의 parametric 부분을 가지고 있다. 1) embedding network $E : O \rightarrow R^n$ 2) comparator network $C : R^n \times R^n \rightarrow [0, 1]$

2개의 parametric 부분은 reachability network의 한 부분으로써 추론하기 위해 함께 학습한다. 아래의 그림에서 볼 수 있듯이.

<p align="center">
  <img src="/images/EPISODIC CURIOSITY THROUGH REACHABILITY/image2.png">
</p>

2개의 non-parameteric 부분은 다음과 같다. 1) episodic memory buffur $M$ 2) reward bonus estination function $B$

EC 모듈의 Overview는 아래의 그림과 같다.

<p align="center">
  <img src="/images/EPISODIC CURIOSITY THROUGH REACHABILITY/image3.png">
</p>

* **Embedding and comparator networks** : 두 개의 네트워크는 observation $o_i$로 부터 $o_j$에 도달하는데까지 걸리는 k-step-reachability를 측정하는데 사용되며 이를 수식적으로 표현하면 $R(o_i,o_j)=C(E(o_i), E(o_j))$가 된다. Figure 2에서 볼 수 있듯이 R-network는 logistic regression loss로 학습된 classifier이다. 특정 k 단계안에 $o_i$로부터 $o_j$로 도달할 수 있으면 0을 출력하고 그렇지 않으면 1을 출력한다. 이 네트워크는 R-network로 칭한다.

* **Episodic memory** : 현재의 에피소드에서 과거 observation들의 embedding정보들을 모으는 buffer M(embedding network E로 연산됩니다)가 있다. Memory buffer M은 메모리와 퍼포먼스 이슈를 해결하기 위해 K 사이즈로 제한된다. 매 스텝에서, 현재 obervation의 embedding 정보가 메모리에 저장된다. 만약 K 사이즈가 넘어간다면 메모리에서 무작위로 하나의 embedding 정보를 제거하고 현재 observation의 embedding 정보로 대체한다. 이 방법으로 오래 전 스텝의 정보가 메모리에서 사라지고 새로운 정보로 대체될 것이지만 무작위로 샘플링을 하기에 오래 전의 스텝 정보가 무조건적으로 사라지진 않는다.

* **Reward bonus estimation module** : 이 모듈의 목적은 현재 observation에 대해서 reachability를 측정하고 현재 메모리 안에 그 정보가 있지 않다면 큰 bonus reward를 할당한다. reachability는 comparator network를 이용하여 현재 메모리 내에 있는 embedding 정보와 비교하는 것으로 측정된다.

# 2.2 Bonus Computation Algorithm

* 매 스텝에서, embedding network를 이용하여 현재 observation $o$에 대한 embedding vector($e=E(o)$)를 가져온다.
그 embedding vector($e$)는 메모리 버퍼 $M=<e_1,e_2,...,e_{|M|}>$에 있는 embedding 정보들과 comparator network C를 이용하여 비교한다.

* 이 comparator network는 reachability buffer를 $c_i=C(e,e_i)$의 값으로 채운다.
그리고 reachability buffer안에 있는 모든 $c_i$들을 이용하여 novelty를 측정한다.

$$
C(C,e) = F(c_1, ..., c_{|M|})
$$

* 여기서 F는 어떠한 함수이다. 이론적으로 F=max과 같은 함수를 사용하는 것도 좋은 방법이다. 하지만 이것을 이용하면 parametric embedding과 comparator network에서 outliers가 발생한다. 경험적으로 그 값의 90%를 사용하면 잘 작동한다. Curiosity bonus로 $b = B(M,e) = \alpha(\beta - C(M, e))$ 이며 $\alpha$와 $\beta$는 하이퍼파라미터이다. 본 논문에서는 $\beta=0.5$를 사용했고 그러므로써 $b$는 $[-\dfrac{\alpha}{2}, \dfrac{\alpha}{2}]$의 범위를 가지게 된다. 그리고 $\alpha$는 task의 성격에 따라서 달라진다. bonus reward를 계산하고 난 후 observation embedding이 만약 특정한 값($b_{novelty}$)보다 높다면 memory에 넣는다(메모리, 데이터 중복 등등에 의해서).

# 2.3 Reachability Network Training

* 만약 Figure 1과 같은 아키텍쳐가 가능하다면, reachability network가 필요하지 않을 것이고 해석적으로 shortest-path algorithm으로 novelty가 계산이 될 수 있다. 하지만 일반적으로 우리는 하나의 경로에 대한 sequence를 얻을 수 있다. 다행히도, 간단한 sequence에 대해서도 training이 가능하다.

* 이 방법은 Figure 2에 도식화 되어 있다. 이 방법은 $o_1, ..., o_N$ 의 일련의 observations들을 입력으로 가지며 그것들을 쌍으로 가진다.

* 쌍 $(o_i, o_j)$ 는 $\rvert i-j \rvert$ < $k$ 이면 reachable하다고 판단하고 positive한 값을 가진다.

* $\rvert i-j \rvert$ > $\gamma k$ 에는 negative한 값을 가진다. 하이퍼 파라미터 $\gamma$ 는 positive와 negative한 값 사이의 gap을 만들어내는데 필요하다.

* 결국 네트워크는 reachability를 계산하기 위해 logistic regression loss를 학습한다.